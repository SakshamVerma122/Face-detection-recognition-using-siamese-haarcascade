{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Import Dependencies","metadata":{}},{"cell_type":"code","source":"# Import standard dependencies\nimport cv2\nimport os\nimport random\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# plt.imshow() --> Main usage of matplotlib here","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:26.217737Z","iopub.execute_input":"2023-04-12T17:35:26.218218Z","iopub.status.idle":"2023-04-12T17:35:26.227245Z","shell.execute_reply.started":"2023-04-12T17:35:26.218173Z","shell.execute_reply":"2023-04-12T17:35:26.226000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import tensorflow dependencies - Functional API\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:26.569046Z","iopub.execute_input":"2023-04-12T17:35:26.569528Z","iopub.status.idle":"2023-04-12T17:35:26.575287Z","shell.execute_reply.started":"2023-04-12T17:35:26.569489Z","shell.execute_reply":"2023-04-12T17:35:26.574156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Layer class helps us to define a custom layer\n# Con2D --> Convolution \n# Dense --> Fully connected Layer\n# MaxPooling2D --> Perform Maxpooling\n# Input --> Base class(Defines What we are going to pass through to our model and our layer/model compile that all together )\n# Flatten --> Takesall the info from the previous layer and flattens it down to a single dimension (CNN output(2D array) --> Desnse layer(1D array))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:28.319406Z","iopub.execute_input":"2023-04-12T17:35:28.320032Z","iopub.status.idle":"2023-04-12T17:35:28.324862Z","shell.execute_reply.started":"2023-04-12T17:35:28.319994Z","shell.execute_reply":"2023-04-12T17:35:28.323697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Set GPU Growth","metadata":{}},{"cell_type":"code","source":"# By default, Tensorflow allocates all available GPU memory for the training process, which can cause the system to run out of memory if the model is large or the GPU has limited memory.\n# setting the memory growth option to True ensures that Tensorflow will allocate GPU memory dynamically, rather than allocating all of it upfront. ","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:30.423264Z","iopub.execute_input":"2023-04-12T17:35:30.423946Z","iopub.status.idle":"2023-04-12T17:35:30.428458Z","shell.execute_reply.started":"2023-04-12T17:35:30.423876Z","shell.execute_reply":"2023-04-12T17:35:30.427392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Avoid OOM errors by setting GPU Memory Consumption Growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus: \n    tf.config.experimental.set_memory_growth(gpu, True)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:30.875680Z","iopub.execute_input":"2023-04-12T17:35:30.876237Z","iopub.status.idle":"2023-04-12T17:35:30.881767Z","shell.execute_reply.started":"2023-04-12T17:35:30.876201Z","shell.execute_reply":"2023-04-12T17:35:30.880654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 Create Folder Structures","metadata":{}},{"cell_type":"markdown","source":"#### 1.4.1 This is for Windows","metadata":{}},{"cell_type":"code","source":"# Setup paths\nPOS_PATH = os.path.join('data', 'positive')\nNEG_PATH = os.path.join('data', 'negative')\nANC_PATH = os.path.join('data', 'anchor')","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:34.268709Z","iopub.execute_input":"2023-04-12T17:35:34.269399Z","iopub.status.idle":"2023-04-12T17:35:34.274668Z","shell.execute_reply.started":"2023-04-12T17:35:34.269358Z","shell.execute_reply":"2023-04-12T17:35:34.273322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make the directories\nos.makedirs(POS_PATH)\nos.makedirs(NEG_PATH)\nos.makedirs(ANC_PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.4.2 This is for Kaggle","metadata":{}},{"cell_type":"code","source":"# input image --> Anchor image\n# Verification image(which is similar) --> +ve image\n# Verification image(which is not similar) --> -ve image\n# -ve images --> labeled faces(repo)\n# Setup paths\nPOS_PATH = '//kaggle//input//positive'\nNEG_PATH = '//kaggle//input//negative'\nANC_PATH = '//kaggle//input//anchor'","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:38.767392Z","iopub.execute_input":"2023-04-12T17:35:38.768151Z","iopub.status.idle":"2023-04-12T17:35:38.774364Z","shell.execute_reply.started":"2023-04-12T17:35:38.768111Z","shell.execute_reply":"2023-04-12T17:35:38.772009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Folder path for +ve image :{POS_PATH}\")\nprint(f\"Folder path for +ve image :{NEG_PATH}\")\nprint(f\"Folder path for +ve image :{ANC_PATH}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:42.942224Z","iopub.execute_input":"2023-04-12T17:35:42.943023Z","iopub.status.idle":"2023-04-12T17:35:42.949498Z","shell.execute_reply.started":"2023-04-12T17:35:42.942979Z","shell.execute_reply":"2023-04-12T17:35:42.948012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Collect Positives and Anchors","metadata":{}},{"cell_type":"code","source":"# http://vis-www.cs.umass.edu/lfw/\n# Uncompress Tar GZ Labelled Faces in the Wild Dataset\n!tar -xf /kaggle/input/labelled-faces-in-the-wild/lfw.tgz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move LFW Images to the following repository data/negative\nfor directory in os.listdir('lfw'):\n    for file in os.listdir(os.path.join('lfw', directory)):\n        EX_PATH = os.path.join('lfw', directory, file)\n        NEW_PATH = os.path.join(NEG_PATH, file)\n        os.replace(EX_PATH, NEW_PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Collect Positive and Anchor Classes","metadata":{}},{"cell_type":"code","source":"# Import uuid library to generate unique image names\nimport uuid\nos.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:35:48.892431Z","iopub.execute_input":"2023-04-12T17:35:48.893404Z","iopub.status.idle":"2023-04-12T17:35:48.902638Z","shell.execute_reply.started":"2023-04-12T17:35:48.893320Z","shell.execute_reply":"2023-04-12T17:35:48.901553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Establish a connection to the webcam\ncap = cv2.VideoCapture(0)\nwhile cap.isOpened(): \n    ret, frame = cap.read()\n   \n    # Cut down frame to 250x250px as we have to train it accordingly to negative\n    # frame = frame[:250,:250,:] --> We didn't took this as it's cutting image from \n    # the starting and in the camera we are facing some where in the middle normally    \n    # 120 is near middle of the image and we went till 120+250 and 200 + 250 as we want something in betweem    \n    frame = frame[120:120+250,200:200+250, :]\n    \n    \n    # Collect anchors \n    if cv2.waitKey(1) & 0XFF == ord('a'):\n        # Create the unique file path \n        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n        # Write out anchor image\n        cv2.imwrite(imgname, frame)\n    \n    # Collect positives\n    if cv2.waitKey(1) & 0XFF == ord('p'):\n        # Create the unique file path \n        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n        \n        # Write out positive image\n        cv2.imwrite(imgname, frame)\n    \n    # Show image back to screen what it's tracking\n    cv2.imshow('Image Collection', frame)\n    \n    # Breaking gracefully\n    # Here the screen is stopped for 1 millisecond and the image is grabbed in that time period only    \n    if cv2.waitKey(1) & 0XFF == ord('q'):\n        break\n        \n# Release the webcam\ncap.release()\n# Close the image show frame\ncv2.destroyAllWindows()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(frame[:,:, :])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Load and Preprocess Images","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Get Image Directories","metadata":{}},{"cell_type":"code","source":"#  Grab all the different mages in a particular directory\n#  Creates a data generator / using a generator then loop through and grab all the files within that specific dictionary / specific directory\n#  Grabs all the specific files with .jpg extension in the back\n#  Helps in batch processing will implement batchwise\nanchor = tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(300)\npositive = tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(300)\nnegative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(300)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:36:43.648654Z","iopub.execute_input":"2023-04-12T17:36:43.649090Z","iopub.status.idle":"2023-04-12T17:36:44.079698Z","shell.execute_reply.started":"2023-04-12T17:36:43.649051Z","shell.execute_reply":"2023-04-12T17:36:44.078361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is basically making a itterator to itterate over the anchor image files\ndir_test = anchor.as_numpy_iterator()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:36:45.859545Z","iopub.execute_input":"2023-04-12T17:36:45.860234Z","iopub.status.idle":"2023-04-12T17:36:45.942511Z","shell.execute_reply.started":"2023-04-12T17:36:45.860192Z","shell.execute_reply":"2023-04-12T17:36:45.941536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full path to a specific image within a particular directory\n# It's an itterator\nprint(dir_test.next())","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:36:48.201132Z","iopub.execute_input":"2023-04-12T17:36:48.201518Z","iopub.status.idle":"2023-04-12T17:36:48.213032Z","shell.execute_reply.started":"2023-04-12T17:36:48.201482Z","shell.execute_reply":"2023-04-12T17:36:48.211767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Preprocessing - Scale and Resize","metadata":{}},{"cell_type":"markdown","source":"### Need of Scaling\n* Machine learning algorithms and Deep learning models require the input data to be normalized, which means that the values are scaled to a specific range, often [0, 1] or [-1, 1].\n* Normalizing the image to the range [0, 1] increases the memory usage by a factor of 4.\n* Scaling the image to the range [0, 1] before storing it, we can avoid the need to divide each pixel value by 255 during training,","metadata":{}},{"cell_type":"code","source":"def preprocess(file_path):\n    \n    # Read in image from file path\n    byte_img = tf.io.read_file(file_path)\n    # Load in the image \n    img = tf.io.decode_jpeg(byte_img)\n    print(\"Before Preprocessing\")\n    print(img)\n    \n    # Preprocessing steps - resizing the image to be 100x100x3(3 is beacuse of RGB seperate matrixes)\n    # It do    \n    \n    # We are resizing it as in the paper it's written it should be 105x 105\n    img = tf.image.resize(img, (100,100))\n    print(img)\n    # Scale image to be between 0 and 1 \n    img = img / 255.0\n    \n    print(\"After Preprocessing\")\n    print(img)\n    # Return image\n    return img","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:38:21.955788Z","iopub.execute_input":"2023-04-12T17:38:21.956993Z","iopub.status.idle":"2023-04-12T17:38:21.963475Z","shell.execute_reply.started":"2023-04-12T17:38:21.956943Z","shell.execute_reply":"2023-04-12T17:38:21.962369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def withoutpreprocessing(file_path):\n     # Read in image from file path(in bytes)\n    byte_img = tf.io.read_file(file_path)\n    \n    # Load in the image \n    img = tf.io.decode_jpeg(byte_img)\n    \n    img = tf.image.resize(img, (100,100))\n    return img","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:38:03.772344Z","iopub.execute_input":"2023-04-12T17:38:03.772997Z","iopub.status.idle":"2023-04-12T17:38:03.779100Z","shell.execute_reply.started":"2023-04-12T17:38:03.772955Z","shell.execute_reply":"2023-04-12T17:38:03.777994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This works only for non scaled image as non scaled image will give same output when used with plt.imshow though the values lie in the range of 0-1\ndef check_images_same(image1,image2):\n    difference = np.abs(image1.numpy()- image2.numpy())\n    if np.sum(difference) == 0:\n        return True\n    else:\n        return False","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:38:07.945457Z","iopub.execute_input":"2023-04-12T17:38:07.946457Z","iopub.status.idle":"2023-04-12T17:38:07.951955Z","shell.execute_reply.started":"2023-04-12T17:38:07.946412Z","shell.execute_reply":"2023-04-12T17:38:07.950738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.2.1 Image after preprocessing","metadata":{}},{"cell_type":"code","source":"img = preprocess('//kaggle//input//anchor/4a74e410-ca6e-11ed-a96a-f47b0953a0e7.jpg')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-12T17:38:24.446386Z","iopub.execute_input":"2023-04-12T17:38:24.447009Z","iopub.status.idle":"2023-04-12T17:38:24.463005Z","shell.execute_reply.started":"2023-04-12T17:38:24.446971Z","shell.execute_reply":"2023-04-12T17:38:24.461733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.2.2 Checking whether scaling os apropriately performed or not","metadata":{}},{"cell_type":"code","source":"print(f\"Maximum pixel val: {img.numpy().max()}\\nMinimum pixel val:{img.numpy().min()}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:38:27.392224Z","iopub.execute_input":"2023-04-12T17:38:27.392693Z","iopub.status.idle":"2023-04-12T17:38:27.399906Z","shell.execute_reply.started":"2023-04-12T17:38:27.392648Z","shell.execute_reply":"2023-04-12T17:38:27.398658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow() expects pixel values to be in the range of [0, 255], so it automatically \n# applies a normalization step to the pixel values of the input image(for viewing purpose only) \n# else the image looks darker and has lower contrast.\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:38:29.596612Z","iopub.execute_input":"2023-04-12T17:38:29.597105Z","iopub.status.idle":"2023-04-12T17:38:29.836674Z","shell.execute_reply.started":"2023-04-12T17:38:29.597068Z","shell.execute_reply":"2023-04-12T17:38:29.835578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.2.3 Image without preprocessing","metadata":{}},{"cell_type":"code","source":"# It will show same image as it automatically scales it back to [0,255]\nplt.imshow(withoutpreprocessing('//kaggle//input//anchor/4a74e410-ca6e-11ed-a96a-f47b0953a0e7.jpg'))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:38:33.895258Z","iopub.execute_input":"2023-04-12T17:38:33.896246Z","iopub.status.idle":"2023-04-12T17:38:34.105772Z","shell.execute_reply.started":"2023-04-12T17:38:33.896191Z","shell.execute_reply":"2023-04-12T17:38:34.104779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It's comming out to be false as we are comparing pixel by pixel\ncheck_images_same(img,withoutpreprocessing('//kaggle//input//anchor/4a74e410-ca6e-11ed-a96a-f47b0953a0e7.jpg'))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:39:22.417721Z","iopub.execute_input":"2023-04-12T17:39:22.418897Z","iopub.status.idle":"2023-04-12T17:39:22.430581Z","shell.execute_reply.started":"2023-04-12T17:39:22.418826Z","shell.execute_reply":"2023-04-12T17:39:22.429444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Load image as a numpy array in the range [0, 1]\nimage = preprocess('//kaggle//input//anchor/4a74e410-ca6e-11ed-a96a-f47b0953a0e7.jpg')\n\n# Display image without automatic scaling to [0, 255]\nfig, ax = plt.subplots()\nim = ax.imshow(image, vmin=0, vmax=1) # cmap='gray' for grayscale images\nfig.colorbar(im)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:39:39.900689Z","iopub.execute_input":"2023-04-12T17:39:39.901298Z","iopub.status.idle":"2023-04-12T17:39:40.159221Z","shell.execute_reply.started":"2023-04-12T17:39:39.901260Z","shell.execute_reply":"2023-04-12T17:39:40.158098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Create Labelled Dataset","metadata":{}},{"cell_type":"code","source":"#       INPUTS           OUTPUTS\n# (anchor, positive) => 1,1,1,1,1\n# (anchor, negative) => 0,0,0,0,0","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:39:44.980202Z","iopub.execute_input":"2023-04-12T17:39:44.981180Z","iopub.status.idle":"2023-04-12T17:39:44.986219Z","shell.execute_reply.started":"2023-04-12T17:39:44.981141Z","shell.execute_reply":"2023-04-12T17:39:44.984886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\nnegatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n\n# Data is the dataset\ndata = positives.concatenate(negatives)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:39:55.420818Z","iopub.execute_input":"2023-04-12T17:39:55.421525Z","iopub.status.idle":"2023-04-12T17:39:55.442705Z","shell.execute_reply.started":"2023-04-12T17:39:55.421486Z","shell.execute_reply":"2023-04-12T17:39:55.441768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Runnng an itterator over the data\nsamples = data.as_numpy_iterator()\nexample = samples.next()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:59:59.364189Z","iopub.execute_input":"2023-04-12T17:59:59.364556Z","iopub.status.idle":"2023-04-12T18:00:18.138049Z","shell.execute_reply.started":"2023-04-12T17:59:59.364521Z","shell.execute_reply":"2023-04-12T18:00:18.136946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.4 Build Train and Test Partition","metadata":{}},{"cell_type":"code","source":"# Making a preprocess_twin function to apply preprocessing function over the two images\ndef preprocess_twin(input_img, validation_img, label):\n    print(type(input_img),type(validation_img),type(label))\n    return(preprocess(input_img), preprocess(validation_img), label)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:07.053498Z","iopub.execute_input":"2023-04-12T17:40:07.053872Z","iopub.status.idle":"2023-04-12T17:40:07.059679Z","shell.execute_reply.started":"2023-04-12T17:40:07.053838Z","shell.execute_reply":"2023-04-12T17:40:07.058649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking whether it's working correctly\nres = preprocess_twin(*example)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:07.528365Z","iopub.execute_input":"2023-04-12T17:40:07.529120Z","iopub.status.idle":"2023-04-12T17:40:07.562130Z","shell.execute_reply.started":"2023-04-12T17:40:07.529082Z","shell.execute_reply":"2023-04-12T17:40:07.561018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4.1 This is for checking whether the preprocess_twin function working correctly ","metadata":{}},{"cell_type":"code","source":"# Input image\nplt.imshow(res[1])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-12T17:40:13.933482Z","iopub.execute_input":"2023-04-12T17:40:13.934418Z","iopub.status.idle":"2023-04-12T17:40:14.148988Z","shell.execute_reply.started":"2023-04-12T17:40:13.934363Z","shell.execute_reply":"2023-04-12T17:40:14.147937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation image\nplt.imshow(res[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:15.521291Z","iopub.execute_input":"2023-04-12T17:40:15.521912Z","iopub.status.idle":"2023-04-12T17:40:15.967026Z","shell.execute_reply.started":"2023-04-12T17:40:15.521854Z","shell.execute_reply":"2023-04-12T17:40:15.965859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label for the above two images\nres[2]","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:16.022426Z","iopub.execute_input":"2023-04-12T17:40:16.023054Z","iopub.status.idle":"2023-04-12T17:40:16.030218Z","shell.execute_reply.started":"2023-04-12T17:40:16.023011Z","shell.execute_reply":"2023-04-12T17:40:16.029123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code for determining the batch size appropriate for the particular machine","metadata":{}},{"cell_type":"code","source":"# Build dataloader pipeline\n# Appling preprocess function to each row of the data\ndata = data.map(preprocess_twin)\n\n# Clearing the cache\ndata = data.cache()\n\n# We shuffling it up\ndata = data.shuffle(buffer_size=1024)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:21.793316Z","iopub.execute_input":"2023-04-12T17:40:21.796608Z","iopub.status.idle":"2023-04-12T17:40:21.933909Z","shell.execute_reply.started":"2023-04-12T17:40:21.796569Z","shell.execute_reply":"2023-04-12T17:40:21.932806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the length of data","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:25.576587Z","iopub.execute_input":"2023-04-12T17:40:25.577667Z","iopub.status.idle":"2023-04-12T17:40:25.582147Z","shell.execute_reply.started":"2023-04-12T17:40:25.577622Z","shell.execute_reply":"2023-04-12T17:40:25.580967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training partition taking 70% data for training purposes\ntrain_data = data.take(round(len(data)*.7))\n\n#  create batches of data with a batch size of 16. This means that the \n# dataset will be divided into chunks of 16 samples each, and each chunk \n# will be fed to the machine learning or deep learning model as one batch \n# during training.\ntrain_data = train_data.batch(16)\n\n# Prefetching\ntrain_data = train_data.prefetch(8)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:30.625684Z","iopub.execute_input":"2023-04-12T17:40:30.626659Z","iopub.status.idle":"2023-04-12T17:40:30.641198Z","shell.execute_reply.started":"2023-04-12T17:40:30.626619Z","shell.execute_reply":"2023-04-12T17:40:30.640203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### USE OF PRE-FETCHING\n*  enables pre-fetching of a specified number of elements from the dataset.\n* improve the training performance of machine learning models by reducing the idle time between iterations. \n* improve the training performance of machine learning models by reducing the idle time between iterations. \n* In general, a larger number of elements can improve training performance by reducing the idle time between batches. However, prefetching too many elements can lead to memory overflow or slow down the training process.\n* A good starting point is to set the prefetch buffer size to the number of elements that can be processed by the model in one iteration, also known as the batch size. For example, if the batch size is set to 32, you can set the prefetch buffer size to 32 or a multiple of 32, such as 64 or 128.","metadata":{}},{"cell_type":"code","source":"# Testing partition\n\n# This will skip first 70% data\ntest_data = data.skip(round(len(data)*.7))\n\n\ntest_data = test_data.take(round(len(data)*.3))\ntest_data = test_data.batch(16)\ntest_data = test_data.prefetch(8)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:40.882555Z","iopub.execute_input":"2023-04-12T17:40:40.883231Z","iopub.status.idle":"2023-04-12T17:40:40.894198Z","shell.execute_reply.started":"2023-04-12T17:40:40.883191Z","shell.execute_reply":"2023-04-12T17:40:40.893044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general, a larger number of elements can improve training performance by reducing the idle time between batches. However, prefetching too many elements can lead to memory overflow or slow down the training process.\n\nA good starting point is to set the prefetch buffer size to the number of elements that can be processed by the model in one iteration, also known as the batch size. For example, if the batch size is set to 32, you can set the prefetch buffer size to 32 or a multiple of 32, such as 64 or 128.","metadata":{}},{"cell_type":"markdown","source":"# 4. Model Engineering","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Build Embedding Layer","metadata":{}},{"cell_type":"code","source":"def make_embedding(): \n    inp = Input(shape=(100,100,3), name='input_image')\n    \n    # First block\n    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n    \n    # Second block\n    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n    print(c2)\n    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n    \n    # Third block \n    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n    \n    # Final embedding block\n    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n    f1 = Flatten()(c4)\n    d1 = Dense(4096, activation='sigmoid')(f1)\n    \n    \n    return Model(inputs=[inp], outputs=[d1], name='embedding')","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:46.911203Z","iopub.execute_input":"2023-04-12T17:40:46.912213Z","iopub.status.idle":"2023-04-12T17:40:46.922860Z","shell.execute_reply.started":"2023-04-12T17:40:46.912159Z","shell.execute_reply":"2023-04-12T17:40:46.921795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding = make_embedding()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:52.713220Z","iopub.execute_input":"2023-04-12T17:40:52.714014Z","iopub.status.idle":"2023-04-12T17:40:52.852661Z","shell.execute_reply.started":"2023-04-12T17:40:52.713973Z","shell.execute_reply":"2023-04-12T17:40:52.851678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-12T17:40:53.417695Z","iopub.execute_input":"2023-04-12T17:40:53.418061Z","iopub.status.idle":"2023-04-12T17:40:53.454495Z","shell.execute_reply.started":"2023-04-12T17:40:53.418028Z","shell.execute_reply":"2023-04-12T17:40:53.453714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Build Distance Layer","metadata":{}},{"cell_type":"code","source":"# Siamese L1 Distance class\n# Defining custom nerual network class\n# It's inheriting Layer class\nclass L1Dist(Layer):\n    \n    # Init method - inheritance\n    def __init__(self, **kwargs):\n        super().__init__()\n       \n    # Magic happens here - similarity calculation\n    def call(self, input_embedding, validation_embedding):\n        return tf.math.abs(input_embedding - validation_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:40:58.800157Z","iopub.execute_input":"2023-04-12T17:40:58.801191Z","iopub.status.idle":"2023-04-12T17:40:58.807779Z","shell.execute_reply.started":"2023-04-12T17:40:58.801136Z","shell.execute_reply":"2023-04-12T17:40:58.806500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1 = L1Dist()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:41:12.392439Z","iopub.execute_input":"2023-04-12T17:41:12.393362Z","iopub.status.idle":"2023-04-12T17:41:12.399741Z","shell.execute_reply.started":"2023-04-12T17:41:12.393308Z","shell.execute_reply":"2023-04-12T17:41:12.398436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 Make Siamese Model","metadata":{}},{"cell_type":"code","source":"def make_siamese_model(): \n    \n    # Anchor image input in the network\n    input_image = Input(name='input_img', shape=(100,100,3))\n    \n    # Validation image in the network \n    validation_image = Input(name='validation_img', shape=(100,100,3))\n    \n    # Combine siamese distance components\n    siamese_layer = L1Dist()\n    siamese_layer._name = 'distance'\n    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n    \n    # Classification layer \n    classifier = Dense(1, activation='sigmoid')(distances)\n    \n    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:42:49.862459Z","iopub.execute_input":"2023-04-12T17:42:49.862971Z","iopub.status.idle":"2023-04-12T17:42:49.870318Z","shell.execute_reply.started":"2023-04-12T17:42:49.862851Z","shell.execute_reply":"2023-04-12T17:42:49.868688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_image = Input(name='input_img', shape=(100,100,3))\nvalidation_image = Input(name='validation_img', shape=(100,100,3))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:42:57.227064Z","iopub.execute_input":"2023-04-12T17:42:57.227436Z","iopub.status.idle":"2023-04-12T17:42:57.234999Z","shell.execute_reply.started":"2023-04-12T17:42:57.227400Z","shell.execute_reply":"2023-04-12T17:42:57.233944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_embedding = embedding(input_image)\nval_embedding = embedding(validation_image)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:02.015700Z","iopub.execute_input":"2023-04-12T17:43:02.016103Z","iopub.status.idle":"2023-04-12T17:43:02.075762Z","shell.execute_reply.started":"2023-04-12T17:43:02.016068Z","shell.execute_reply":"2023-04-12T17:43:02.074743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_layer = L1Dist()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:05.327733Z","iopub.execute_input":"2023-04-12T17:43:05.328449Z","iopub.status.idle":"2023-04-12T17:43:05.333944Z","shell.execute_reply.started":"2023-04-12T17:43:05.328407Z","shell.execute_reply":"2023-04-12T17:43:05.332863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distances = siamese_layer(inp_embedding, val_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:07.668486Z","iopub.execute_input":"2023-04-12T17:43:07.669422Z","iopub.status.idle":"2023-04-12T17:43:07.707509Z","shell.execute_reply.started":"2023-04-12T17:43:07.669366Z","shell.execute_reply":"2023-04-12T17:43:07.706543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = Dense(1, activation='sigmoid')(distances)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:08.107875Z","iopub.execute_input":"2023-04-12T17:43:08.108586Z","iopub.status.idle":"2023-04-12T17:43:08.130953Z","shell.execute_reply.started":"2023-04-12T17:43:08.108551Z","shell.execute_reply":"2023-04-12T17:43:08.129876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:10.404523Z","iopub.execute_input":"2023-04-12T17:43:10.405397Z","iopub.status.idle":"2023-04-12T17:43:10.416173Z","shell.execute_reply.started":"2023-04-12T17:43:10.405352Z","shell.execute_reply":"2023-04-12T17:43:10.415046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_network = Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:38.906056Z","iopub.execute_input":"2023-04-12T17:43:38.906627Z","iopub.status.idle":"2023-04-12T17:43:38.916331Z","shell.execute_reply.started":"2023-04-12T17:43:38.906589Z","shell.execute_reply":"2023-04-12T17:43:38.914922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_network.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:40.334231Z","iopub.execute_input":"2023-04-12T17:43:40.334594Z","iopub.status.idle":"2023-04-12T17:43:40.367479Z","shell.execute_reply.started":"2023-04-12T17:43:40.334560Z","shell.execute_reply":"2023-04-12T17:43:40.366655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_model = make_siamese_model()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:40.745037Z","iopub.execute_input":"2023-04-12T17:43:40.746114Z","iopub.status.idle":"2023-04-12T17:43:40.818245Z","shell.execute_reply.started":"2023-04-12T17:43:40.746073Z","shell.execute_reply":"2023-04-12T17:43:40.817320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:42.225413Z","iopub.execute_input":"2023-04-12T17:43:42.226145Z","iopub.status.idle":"2023-04-12T17:43:42.255966Z","shell.execute_reply.started":"2023-04-12T17:43:42.226103Z","shell.execute_reply":"2023-04-12T17:43:42.255161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SET UP LOSS FUNCTION AND OPTIMISER","metadata":{}},{"cell_type":"code","source":"binary_cross_loss = tf.losses.BinaryCrossentropy()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:48.508769Z","iopub.execute_input":"2023-04-12T17:43:48.509957Z","iopub.status.idle":"2023-04-12T17:43:48.515816Z","shell.execute_reply.started":"2023-04-12T17:43:48.509875Z","shell.execute_reply":"2023-04-12T17:43:48.514539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(1e-4) # 0.0001","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:52.105201Z","iopub.execute_input":"2023-04-12T17:43:52.106248Z","iopub.status.idle":"2023-04-12T17:43:52.117206Z","shell.execute_reply.started":"2023-04-12T17:43:52.106178Z","shell.execute_reply":"2023-04-12T17:43:52.116148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checkpoints","metadata":{}},{"cell_type":"code","source":"checkpoint_dir = '.\\training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\ncheckpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:57.631642Z","iopub.execute_input":"2023-04-12T17:43:57.632382Z","iopub.status.idle":"2023-04-12T17:43:57.638040Z","shell.execute_reply.started":"2023-04-12T17:43:57.632336Z","shell.execute_reply":"2023-04-12T17:43:57.636810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting a batch of 16 rows via firstly converting it as a numpy_iterator\ntest_batch = train_data.as_numpy_iterator()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:58.217601Z","iopub.execute_input":"2023-04-12T17:43:58.218263Z","iopub.status.idle":"2023-04-12T17:43:58.249966Z","shell.execute_reply.started":"2023-04-12T17:43:58.218225Z","shell.execute_reply":"2023-04-12T17:43:58.248998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Geting the batch\nbatch_1 = test_batch.next()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:44:00.926166Z","iopub.execute_input":"2023-04-12T17:44:00.926545Z","iopub.status.idle":"2023-04-12T17:44:05.174042Z","shell.execute_reply.started":"2023-04-12T17:44:00.926509Z","shell.execute_reply":"2023-04-12T17:44:05.172541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our valiation and (positive/negative images) are at index 1 and 0 resp\nX = batch_1[:2]","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:44:05.176549Z","iopub.execute_input":"2023-04-12T17:44:05.177173Z","iopub.status.idle":"2023-04-12T17:44:05.186487Z","shell.execute_reply.started":"2023-04-12T17:44:05.177134Z","shell.execute_reply":"2023-04-12T17:44:05.181955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is for label\ny = batch_1[2]","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:44:05.188038Z","iopub.execute_input":"2023-04-12T17:44:05.188685Z","iopub.status.idle":"2023-04-12T17:44:05.199201Z","shell.execute_reply.started":"2023-04-12T17:44:05.188647Z","shell.execute_reply":"2023-04-12T17:44:05.196868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:44:05.202905Z","iopub.execute_input":"2023-04-12T17:44:05.203662Z","iopub.status.idle":"2023-04-12T17:44:05.214141Z","shell.execute_reply.started":"2023-04-12T17:44:05.203623Z","shell.execute_reply":"2023-04-12T17:44:05.213062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.losses.BinaryCrossentropy??","metadata":{"execution":{"iopub.status.busy":"2023-04-09T05:55:11.978207Z","iopub.execute_input":"2023-04-09T05:55:11.979544Z","iopub.status.idle":"2023-04-09T05:55:12.044481Z","shell.execute_reply.started":"2023-04-09T05:55:11.979506Z","shell.execute_reply":"2023-04-09T05:55:12.043470Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(batch):\n    \n    # Record all of our operations \n    with tf.GradientTape() as tape:     \n        # Get anchor and positive/negative image\n        X = batch[:2]\n        # Get label\n        y = batch[2]\n        \n        # Forward pass\n        yhat = siamese_model(X, training=True)\n        # Calculate loss\n        loss = binary_cross_loss(y, yhat)\n    print(loss)\n        \n    # Calculate gradients\n    grad = tape.gradient(loss, siamese_model.trainable_variables)\n    \n    # Calculate updated weights and apply to siamese model\n    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n        \n    # Return loss\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-04-09T05:55:12.045999Z","iopub.execute_input":"2023-04-09T05:55:12.046355Z","iopub.status.idle":"2023-04-09T05:55:12.053350Z","shell.execute_reply.started":"2023-04-09T05:55:12.046319Z","shell.execute_reply":"2023-04-09T05:55:12.052261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import metric calculations\nfrom tensorflow.keras.metrics import Precision, Recall","metadata":{"execution":{"iopub.status.busy":"2023-04-09T05:55:12.054989Z","iopub.execute_input":"2023-04-09T05:55:12.055336Z","iopub.status.idle":"2023-04-09T05:55:12.065311Z","shell.execute_reply.started":"2023-04-09T05:55:12.055302Z","shell.execute_reply":"2023-04-09T05:55:12.064291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(data, EPOCHS):\n    # Loop through epochs\n    for epoch in range(1, EPOCHS+1):\n        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n        progbar = tf.keras.utils.Progbar(len(data))\n        \n        # Creating a metric object \n        r = Recall()\n        p = Precision()\n        \n        # Loop through each batch\n        for idx, batch in enumerate(data):\n            # Run train step here\n            loss = train_step(batch)\n            yhat = siamese_model.predict(batch[:2])\n            r.update_state(batch[2], yhat)\n            p.update_state(batch[2], yhat) \n            progbar.update(idx+1)\n        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n        \n        # Save checkpoints\n        if epoch % 10 == 0: \n            checkpoint.save(file_prefix=checkpoint_prefix)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T05:55:17.695203Z","iopub.execute_input":"2023-04-09T05:55:17.696197Z","iopub.status.idle":"2023-04-09T05:55:17.704191Z","shell.execute_reply.started":"2023-04-09T05:55:17.696159Z","shell.execute_reply":"2023-04-09T05:55:17.703037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 50\ntrain(train_data, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T05:55:18.417923Z","iopub.execute_input":"2023-04-09T05:55:18.418333Z","iopub.status.idle":"2023-04-09T06:02:46.593636Z","shell.execute_reply.started":"2023-04-09T05:55:18.418294Z","shell.execute_reply":"2023-04-09T06:02:46.592154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Evaluate the Model","metadata":{}},{"cell_type":"markdown","source":"#### 6.1 Import Metrics","metadata":{}},{"cell_type":"code","source":"# Import metric calculations\nfrom tensorflow.keras.metrics import Precision, Recall","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:12.061213Z","iopub.execute_input":"2023-04-09T06:03:12.061601Z","iopub.status.idle":"2023-04-09T06:03:12.066854Z","shell.execute_reply.started":"2023-04-09T06:03:12.061565Z","shell.execute_reply":"2023-04-09T06:03:12.065671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a batch of test data\ntest_input, test_val, y_true = test_data.as_numpy_iterator().next()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:13.559724Z","iopub.execute_input":"2023-04-09T06:03:13.560032Z","iopub.status.idle":"2023-04-09T06:03:13.585559Z","shell.execute_reply.started":"2023-04-09T06:03:13.560003Z","shell.execute_reply":"2023-04-09T06:03:13.584612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat = siamese_model.predict([test_input, test_val])","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:14.674928Z","iopub.execute_input":"2023-04-09T06:03:14.675285Z","iopub.status.idle":"2023-04-09T06:03:14.819473Z","shell.execute_reply.started":"2023-04-09T06:03:14.675252Z","shell.execute_reply":"2023-04-09T06:03:14.818506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Post processing the results \n[1 if prediction > 0.5 else 0 for prediction in y_hat ]","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:20.855489Z","iopub.execute_input":"2023-04-09T06:03:20.855857Z","iopub.status.idle":"2023-04-09T06:03:20.863270Z","shell.execute_reply.started":"2023-04-09T06:03:20.855824Z","shell.execute_reply":"2023-04-09T06:03:20.862188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:23.463608Z","iopub.execute_input":"2023-04-09T06:03:23.463997Z","iopub.status.idle":"2023-04-09T06:03:23.471391Z","shell.execute_reply.started":"2023-04-09T06:03:23.463962Z","shell.execute_reply":"2023-04-09T06:03:23.470272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.3 Calculate Metrics","metadata":{}},{"cell_type":"code","source":"# Creating a metric object \nm = Recall()\n\n# Calculating the recall value \nm.update_state(y_true, y_hat)\n\n# Return Recall Result\nm.result().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:26.108225Z","iopub.execute_input":"2023-04-09T06:03:26.108702Z","iopub.status.idle":"2023-04-09T06:03:26.129209Z","shell.execute_reply.started":"2023-04-09T06:03:26.108662Z","shell.execute_reply":"2023-04-09T06:03:26.128321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a metric object \nm = Precision()\n\n# Calculating the recall value \nm.update_state(y_true, y_hat)\n\n# Return Recall Result\nm.result().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:28.447387Z","iopub.execute_input":"2023-04-09T06:03:28.448091Z","iopub.status.idle":"2023-04-09T06:03:28.466718Z","shell.execute_reply.started":"2023-04-09T06:03:28.448055Z","shell.execute_reply":"2023-04-09T06:03:28.465691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = Recall()\np = Precision()\n\nfor test_input, test_val, y_true in test_data.as_numpy_iterator():\n    yhat = siamese_model.predict([test_input, test_val])\n    r.update_state(y_true, yhat)\n    p.update_state(y_true,yhat) \n\nprint(r.result().numpy(), p.result().numpy())","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:31.367784Z","iopub.execute_input":"2023-04-09T06:03:31.368752Z","iopub.status.idle":"2023-04-09T06:03:32.844487Z","shell.execute_reply.started":"2023-04-09T06:03:31.368701Z","shell.execute_reply":"2023-04-09T06:03:32.843507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.4 Viz Results","metadata":{}},{"cell_type":"code","source":"# Set plot size \nplt.figure(figsize=(10,8))\n\n# Set first subplot\nplt.subplot(1,2,1)\nplt.imshow(test_input[0])\n\n# Set second subplot\nplt.subplot(1,2,2)\nplt.imshow(test_val[0])\n\n# Renders cleanly\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:03:38.872015Z","iopub.execute_input":"2023-04-09T06:03:38.872949Z","iopub.status.idle":"2023-04-09T06:03:39.163476Z","shell.execute_reply.started":"2023-04-09T06:03:38.872894Z","shell.execute_reply":"2023-04-09T06:03:39.162442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Save Model","metadata":{}},{"cell_type":"code","source":"# Save weights\nsiamese_model.save('siamesemodelv2.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:05:40.778285Z","iopub.execute_input":"2023-04-09T06:05:40.779567Z","iopub.status.idle":"2023-04-09T06:05:41.131452Z","shell.execute_reply.started":"2023-04-09T06:05:40.779494Z","shell.execute_reply":"2023-04-09T06:05:41.124314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"L1Dist","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:05:42.110221Z","iopub.execute_input":"2023-04-09T06:05:42.110898Z","iopub.status.idle":"2023-04-09T06:05:42.117634Z","shell.execute_reply.started":"2023-04-09T06:05:42.110858Z","shell.execute_reply":"2023-04-09T06:05:42.116405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload model \nsiamese_model = tf.keras.models.load_model('siamesemodelv2.h5', \n                                   custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:05:43.817384Z","iopub.execute_input":"2023-04-09T06:05:43.818104Z","iopub.status.idle":"2023-04-09T06:05:44.570822Z","shell.execute_reply.started":"2023-04-09T06:05:43.818066Z","shell.execute_reply":"2023-04-09T06:05:44.569718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions with reloaded model\nsiamese_model.predict([test_input, test_val])","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:05:46.250916Z","iopub.execute_input":"2023-04-09T06:05:46.251306Z","iopub.status.idle":"2023-04-09T06:05:46.441958Z","shell.execute_reply.started":"2023-04-09T06:05:46.251270Z","shell.execute_reply":"2023-04-09T06:05:46.440402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View model summary\nsiamese_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T06:06:29.548517Z","iopub.execute_input":"2023-04-09T06:06:29.549578Z","iopub.status.idle":"2023-04-09T06:06:29.573553Z","shell.execute_reply.started":"2023-04-09T06:06:29.549519Z","shell.execute_reply":"2023-04-09T06:06:29.572778Z"},"trusted":true},"execution_count":null,"outputs":[]}]}